<img width="1647" height="1374" alt="image" src="https://github.com/user-attachments/assets/070df42c-376f-45b4-806f-80068d6116d9" /># Cron Root Attention‚Ñ¢

**Sub-quadratic O(N‚àöN) attention with 2-hop relay for long-context transformers**

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.2+](https://img.shields.io/badge/pytorch-2.2+-ee4c2c.svg)](https://pytorch.org/)

## Key Results

Cron Root Attention achieves **up to 57x forward kernel speedup** over SDPA/FlashAttention-2 at long sequence lengths by reducing attention complexity from O(N¬≤) to O(N‚àöN), with **100% token coverage** through a 3-phase relay mechanism. Forward crossover is **~1K tokens**, training crossover is **~2K tokens**. The **hybrid mode** auto-selects SDPA below crossover for zero-regression deployment.

### Forward Pass Benchmarks (Kernel Only)

| Sequence Length | Cron Root | SDPA (Flash) | Speedup |
|-----------------|-----------|--------------|----------|
| 512 | 0.031ms | 0.017ms | 0.53x |
| 1,024 | 0.030ms | 0.031ms | **1.02x** |
| 2,048 | 0.032ms | 0.109ms | **3.44x** |
| 4,096 | 0.032ms | 0.295ms | **9.24x** |
| 8,192 | 0.072ms | 0.966ms | **13.3x** |
| 16,384 | 0.335ms | 3.32ms | **9.91x** |
| 32,768 | 1.19ms | 12.8ms | **10.8x** |
| 65,536 | 2.68ms | 49.7ms | **18.6x** |
| 131,072 | 7.28ms | 197ms | **27.0x** |
| 262,144 | 17.0ms | 772ms | **45.3x** |
| 524,288 | 54.4ms | 3089ms | **56.8x** |

*Measured on RTX 5070 Ti (Blackwell GB203), PyTorch 2.9.1, CUDA 12.8, FP16, B=1, H=8, D=64. CUDA event timing, trimmed mean of 30+ runs.*

### End-to-End Training Performance (Forward + Backward)

| Sequence Length | Cron Root (Fwd+Bwd) | SDPA (Fwd+Bwd) | Training Speedup |
|-----------------|---------------------|----------------|------------------|
| 512 | 0.160ms | 0.087ms | 0.54x |
| 1,024 | 0.155ms | 0.120ms | 0.77x |
| 2,048 | 0.343ms | 0.332ms | 0.97x |
| 4,096 | 0.658ms | 0.954ms | **1.45x** |
| 8,192 | 1.70ms | 2.93ms | **1.73x** |
| 16,384 | 8.66ms | 12.3ms | **1.42x** |
| 32,768 | 27.4ms | 46.3ms | **1.69x** |
| 65,536 | 65.2ms | 173ms | **2.65x** |
| 131,072 | 182ms | 683ms | **3.76x** |

Training crossover is **~2K tokens** (was ~12K before optimization). The backward uses a **single fully-fused kernel** for S ‚â§ 8K that computes dQ + local dK/dV + strided dK/dV in one launch.

### Inference Benchmarks (no_grad prefill)

| Sequence Length | Cron Root | SDPA (Flash) | Inference Speedup |
|-----------------|-----------|--------------|-------------------|
| 512 | 0.031ms | 0.017ms | 0.53x |
| 1,024 | 0.030ms | 0.031ms | **1.02x** |
| 2,048 | 0.032ms | 0.109ms | **3.44x** |
| 4,096 | 0.032ms | 0.295ms | **9.24x** |
| 8,192 | 0.072ms | 0.966ms | **13.3x** |
| 16,384 | 0.335ms | 3.32ms | **9.91x** |
| 32,768 | 1.19ms | 12.8ms | **10.8x** |
| 65,536 | 2.68ms | 49.7ms | **18.6x** |
| 131,072 | 7.28ms | 197ms | **27.0x** |
| 262,144 | 17.0ms | 772ms | **45.3x** |
| 524,288 | 54.4ms | 3089ms | **56.8x** |

> **Note**: The **hybrid mode** (`cron_root_attention_hybrid`) auto-selects SDPA for S < 1536 and Cron Root for S ‚â• 1536, guaranteeing **‚â•1.0x speedup at ALL sequence lengths**. For inference-only workloads, the forward kernel crossover is only ~1K tokens.

## üì¶ Installation

```bash
pip install cron-root-attention
```

Or from source:
```bash
git clone https://github.com/zitacron/cron-root-attention.git
cd cron-root-attention
pip install -e .
```

## ‚ö° Quick Start

### 10-Line Integration

```python
import torch
from cron_root_attention import cron_root_attention

# Your existing Q, K, V tensors (B, H, S, D)
q = torch.randn(1, 16, 8192, 64, device='cuda', dtype=torch.float16)
k = torch.randn(1, 16, 8192, 64, device='cuda', dtype=torch.float16)
v = torch.randn(1, 16, 8192, 64, device='cuda', dtype=torch.float16)

# Drop-in replacement for scaled_dot_product_attention
output = cron_root_attention(q, k, v)  # 13x faster at S=8192!
```

### Module API (Drop-in for nn.MultiheadAttention)

```python
from cron_root_attention import CronRootMultiheadAttention

# Replace nn.MultiheadAttention
attn = CronRootMultiheadAttention(
    embed_dim=1024,
    num_heads=16,
    batch_first=True
).cuda()

x = torch.randn(1, 8192, 1024, device='cuda', dtype=torch.float16)
output, _ = attn(x, x, x)  # Automatic ‚àöN sparse attention
```

### Hybrid Mode (Auto Backend Selection)

Cron Root's sub-quadratic kernels are optimized for longer sequences. For very short sequences (S < 1536), SDPA's hand-tuned CUDA kernels are still faster. The hybrid mode auto-selects the fastest backend at any length ‚Äî guaranteeing **‚â•1.0x speedup always**.

```python
from cron_root_attention import cron_root_attention_hybrid

# Automatically uses SDPA for S < 1536, Cron Root for S >= 1536
output = cron_root_attention_hybrid(q, k, v, backend='auto')
```

## How It Works

### The 3-Phase ‚àöN Sparse Attention Pattern

Instead of attending to all N previous tokens (O(N¬≤)), each query attends to 3‚àöN tokens across three phases:

1. **Phase 1 ‚Äî Local Window (‚àöN tokens)**: Immediate predecessors for fine-grained context
2. **Phase 2 ‚Äî Strided Window (‚àöN tokens)**: Every ‚àöN-th token for global sampling
3. **Phase 3 ‚Äî Relay Keys (‚àöN blocks)**: Block-mean summaries carrying compressed 2-hop information

```
Query at position 100 (‚àöN = 23, S = 512):

  Phase 1 - Local:   [78, 79, ..., 100]              (23 tokens, exact)
  Phase 2 - Strided: [0, 23, 46, 69]                 (4 tokens, exact, before local)
  Phase 3 - Relay:   [block0_mean, block1_mean, ...]  (‚â§23 compressed blocks)
                      each block summarizes 23 tokens

  Softmax slots: 3‚àöN = 69
  Token coverage: 512/512 = 100% (relay blocks cover entire sequence)
```

### 2-Hop Relay Mechanism

The relay mechanism solves the **gradient dilution problem** inherent in multi-hop sparse attention. Without relay, 2-hop information must survive two separate softmax normalizations across layers ‚Äî gradients wash out exponentially.

### Mathematical explanation
"If its sub quadratic compexity, then why isn't it faster than SDPA(Flash) at smaller sequences?"

This is because the more your complexity converges to a linear complexity, the less efficient it is despite being scalable.
Here is an image showing how sequence length (x axis) scales with time (y axis) as it increases
<img width="2726" height="1007" alt="image" src="https://github.com/user-attachments/assets/07f66ac5-a776-4834-ab99-08d7df548d00"/>

This image shows that our N‚àöN complexity IS working as intended, making it scalable as sequence length increases, but what about smaller sequences?

<img width="1647" height="1374" alt="image" src="https://github.com/user-attachments/assets/fbf5b8cf-7355-4c01-a74b-3da6fb13e878" />

So desipite the speedup at longer sequences, there is still a slight overhead for calculating the ‚àö of the sequence. This essentially means we are trading compute for memory, yet compute is so low the net gain is positive.


**Relay solves this by carrying compressed 2-hop information through a single softmax:**

```
Pre-computation (PyTorch, before kernel launch):
  relay_k[r] = mean(K[r¬∑‚àöN : (r+1)¬∑‚àöN])   # Block-mean keys
  relay_v[r] = mean(V[r¬∑‚àöN : (r+1)¬∑‚àöN])   # Block-mean values
  Shape: (B, H, NUM_RELAY, D) where NUM_RELAY ‚âà ‚àöN

Inside the kernel (same softmax as Phase 1 & 2):
  For each query q_m:
    score(q_m, relay_k[r]) participates in the SAME online softmax
    as local and strided scores ‚Äî single normalization, no dilution

Backward (gradient scatter via chain rule):
  dK[r¬∑‚àöN + i] += d_relay_k[r] / ‚àöN   for all i in block r
```

**Result**: Every query sees the entire sequence ‚Äî local positions exactly, strided positions exactly, and all remaining positions through compressed relay blocks. All in O(N‚àöN) complexity.

### Optimized Backward Pass

**Short sequences (S ‚â§ 8192) ‚Äî Fully-Fused Single Kernel:**
- One Triton kernel computes dQ + local dK/dV + strided dK/dV
- Uses `atomic_add` for K/V gradients (strided contributions overlap)
- Relay overhead is skipped entirely (`skip_relay` optimization)
- Reduces backward from 4 kernel launches to **1 kernel launch**

**Long sequences (S > 8192) ‚Äî Key-Centric Multi-Kernel:**
- Each block **owns** one strided key and iterates over all queries
- Zero atomic contention (vs O(N) atomics in query-parallel)
- Register accumulation ‚Üí single write at end
- Relay backward: each block owns one relay key/value pair, gradient scatter via `dK[r¬∑‚àöN+i] += d_relay_k[r] / ‚àöN`

## üìä Benchmarks

### Complexity Comparison

| Method | Time Complexity | Memory | Pattern | Full Coverage |
|--------|----------------|--------|---------|---------------|
| Dense SDPA | O(N¬≤) | O(N) | Full causal | Yes |
| FlashAttention-2 | O(N¬≤) | O(N) | Full causal | Yes |
| Longformer | O(N¬∑w) | O(N) | Local + global | Limited |
| **Cron Root (Ours)** | **O(N‚àöN)** | **O(N)** | **Local + Strided + Relay** | **Yes (2-hop)** |

### Supported Hardware

Automatic SM detection supports 40+ GPU models:

| Category | GPUs |
|----------|------|
| Blackwell (50 series) | RTX 5090, 5080, 5070 Ti, 5070 |
| Ada (40 series) | RTX 4090, 4080, 4070 Ti, 4070, 4060 Ti |
| Ampere (30 series) | RTX 3090, 3080, 3070, 3060 |
| Turing (20 series) | RTX 2080 Ti, 2080, 2070, 2060, TITAN RTX |
| Datacenter | H100, H200, H800, A100, L40S, L4, V100, B100, B200 |

```python
from cron_root_attention import get_gpu_info
print(get_gpu_info())
# {'gpu_name': 'NVIDIA GeForce RTX 5070 Ti', 'sm_count': 70, 'is_known_gpu': True}
```

## Requirements

- Python 3.10+
- PyTorch 2.2+
- Triton 2.2+
- CUDA 12.0+ (Blackwell/Hopper recommended)

## üìÑ Citation

If you use Cron Root Attention in your research, please cite:
```bibtex
@software{cron_root_attention,
  author = {{Zitacron Project}},
  title = {Cron Root Attention: Sub-quadratic Attention for Long-Context Transformers},
  year = {2026},
  url = {https://github.com/zitacron/cron-root-attention},
  version = {0.1.0},
  note = {Zitacron‚Ñ¢ and Cron Root Attention‚Ñ¢ are trademarks of the Zitacron Project.}
}
```

## License

Apache 2.0 - See [LICENSE](LICENSE) for details.
NOTICE - See [NOTICE](NOTICE) for further details.

## üôè Acknowledgments

- Inspired by [FlashAttention](https://github.com/Dao-AILab/flash-attention) by Tri Dao
- Built on [Triton](https://github.com/openai/triton) by OpenAI
- Optimized for NVIDIA Blackwell architecture (GB203) (more to come!)

**Zitacron** - Building the future of efficient AI

---

¬© 2026 Zitacron. "Zitacron" and "Cron Root Attention" are trademarks of the Zitacron Project. 
Optimized for NVIDIA Blackwell Architecture.


