\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}

% ArXiv preprint style
\usepackage[margin=1in]{geometry}

\title{Cron Root Attention: Subquadratic Self-Attention via Structured Sparsity}

\author{
  Zitacron\\
  \texttt{https://github.com/zitacron}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We introduce Cron Root Attention, a sparse attention mechanism that reduces the computational complexity of self-attention from $O(N^2)$ to $O(N\sqrt{N})$ while preserving causal language modeling semantics. Our method combines three phases: (1) a local sliding window of size $\sqrt{N}$, (2) a strided global window of size $\sqrt{N}$, and (3) a \textbf{relay mechanism} that carries block-mean compressed key/value summaries from $\sqrt{N}$ blocks, enabling full-sequence coverage through a single softmax pass. Each query attends to $O(3\sqrt{N})$ positions, achieving 100\% token coverage without gradient dilution. We implement optimized Triton kernels with a fully-fused single-kernel backward pass and relay-skip optimization that achieve up to \textbf{57x forward kernel speedup} at sequence length 524K and \textbf{3.76x end-to-end training speedup} at 131K tokens, with forward crossover at $\sim$1K tokens and training crossover at $\sim$2K tokens. Our implementation supports 40+ GPU models from consumer (RTX 20/30/40/50 series) to datacenter (H100, H800, A100, V100) hardware with automatic SM detection. We release Cron Root Attention as an open-source pip package under Apache 2.0 license.
\end{abstract}

\section{Introduction}

Transformer attention scales quadratically with sequence length, creating a fundamental bottleneck for long-context language models. While Flash Attention \cite{flashattn} and FlashAttention-2 \cite{flashattn2} optimize memory access patterns, they maintain $O(N^2)$ complexity. Sparse attention variants like Longformer \cite{longformer} and BigBird \cite{bigbird} achieve linear complexity but require fixed-size windows that don't scale with context.

We propose Cron Root Attention (named for Zitacron + Root $\sqrt{}$), which achieves true subquadratic scaling by using a window size proportional to $\sqrt{N}$. Our key insight is that combining:
\begin{enumerate}
    \item A \textbf{local window} of size $\sqrt{N}$ for recent context
    \item A \textbf{strided window} sampling every $\sqrt{N}$-th token
    \item A \textbf{relay mechanism} carrying block-mean K/V summaries from $\sqrt{N}$ blocks
\end{enumerate}
yields $O(N\sqrt{N})$ total complexity while providing \textbf{100\% sequence coverage} through a single softmax pass, eliminating the gradient dilution problem inherent in multi-hop sparse attention.

\subsection{Contributions}

\begin{itemize}
    \item We formalize the $\sqrt{N}$ attention pattern and prove its $O(N\sqrt{N})$ complexity with 2-hop universal reachability (Section \ref{sec:method}).
    \item We develop block-query tiled Triton kernels with a fully-fused single-kernel backward achieving 57x forward speedup at 524K tokens (Section \ref{sec:implementation}).
    \item We demonstrate 3.76x end-to-end training speedup at 131K sequence length with training crossover at $\sim$2K tokens (Section \ref{sec:experiments}).
    \item We release a pip-installable package with automatic GPU detection supporting 40+ GPU models (Section \ref{sec:code}).
\end{itemize}

\section{Related Work}

\textbf{Efficient Attention.} Flash Attention \cite{flashattn} uses tiling and recomputation to reduce memory from $O(N^2)$ to $O(N)$ while maintaining exact computation. FlashAttention-2 \cite{flashattn2} improves parallelism. Flash-Decoding \cite{flashdecoding} optimizes inference. These maintain quadratic compute.

\textbf{Sparse Attention.} Longformer \cite{longformer} uses local + global tokens. BigBird \cite{bigbird} adds random attention. Sparse Transformers \cite{sparse_transformers} use strided patterns. These typically use fixed window sizes, limiting scalability.

\textbf{Linear Attention.} Linear attention \cite{linear_attention} and variants like RWKV \cite{rwkv} and Mamba \cite{mamba} achieve $O(N)$ but sacrifice expressivity.

\textbf{Our Approach.} Cron Root Attention combines the hardware optimization of Flash Attention with the scalability of sparse attention, using adaptive $\sqrt{N}$-sized windows that scale with sequence length.

\section{Method}
\label{sec:method}

\subsection{Attention Pattern}

For a sequence of length $N$, we define the window size $W = \lceil\sqrt{N}\rceil$. Each query $q_i$ attends to:

\begin{equation}
\mathcal{A}(i) = \underbrace{\{j : \max(0, i-W+1) \leq j \leq i\}}_{\text{Local window}} \cup \underbrace{\{j : j \equiv 0 \pmod{W}, j < \max(0, i-W+1)\}}_{\text{Strided window}}
\end{equation}

\subsection{Complexity Analysis}

\begin{theorem}
The total attention cost is $O(N\sqrt{N})$.
\end{theorem}

\begin{proof}
Each query attends to at most $W$ local positions and at most $\lceil N/W \rceil$ strided positions:
\begin{equation}
|\mathcal{A}(i)| \leq W + \frac{N}{W} = \sqrt{N} + \sqrt{N} = 2\sqrt{N}
\end{equation}
Summing over $N$ queries: $N \cdot 2\sqrt{N} = O(N\sqrt{N})$.
\end{proof}

\subsection{2-Hop Reachability}

\begin{theorem}
Any token can reach any other token in at most 2 attention hops.
\end{theorem}

\begin{proof}
For tokens $A$ at position $i$ and $B$ at position $j$ where $j < i - W$:
\begin{enumerate}
    \item Let $C$ be at the strided position $\lfloor j/W \rfloor \cdot W$
    \item $A$ attends to $C$ via strided window (since $C < i - W + 1$)
    \item $C$ attends to $B$ via local window (since $|C - j| < W$)
\end{enumerate}
Thus $A \rightarrow C \rightarrow B$ forms a 2-hop path.
\end{proof}

\subsection{Relay Mechanism (Phase 3)}

While the 2-hop reachability property guarantees theoretical coverage, in practice information traversing two separate softmax normalizations across layers suffers from \textbf{gradient dilution}: the gradient signal decays exponentially through each softmax bottleneck.

We introduce a relay mechanism that carries compressed 2-hop information through a \textbf{single} softmax pass. For each block $r \in \{0, 1, \ldots, \lceil N/W \rceil - 1\}$, we pre-compute:
\begin{equation}
\text{relay\_k}[r] = \frac{1}{W} \sum_{j=rW}^{(r+1)W-1} k_j, \quad \text{relay\_v}[r] = \frac{1}{W} \sum_{j=rW}^{(r+1)W-1} v_j
\end{equation}

These relay keys and values participate in the \textbf{same} softmax as the local and strided scores:
\begin{equation}
\mathcal{A}(i) = \mathcal{A}_{\text{local}}(i) \cup \mathcal{A}_{\text{strided}}(i) \cup \mathcal{A}_{\text{relay}}(i)
\end{equation}
where $\mathcal{A}_{\text{relay}}(i) = \{r : (r+1)W - 1 < \max(0, i - W + 1)\}$, i.e., relay blocks whose entire span precedes the local window.

The total attention set size remains $O(3\sqrt{N})$ per query:
\begin{equation}
|\mathcal{A}(i)| \leq W + \frac{N}{W} + \frac{N}{W} = 3\sqrt{N}
\end{equation}

The relay backward pass uses the mean-pooling Jacobian for gradient scatter:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial k_j} \mathrel{+}= \frac{1}{W} \cdot \frac{\partial \mathcal{L}}{\partial \text{relay\_k}[\lfloor j/W \rfloor]}
\end{equation}

This is implemented as a zero-atomic, exclusive-ownership relay $dK/dV$ kernel where each block owns one relay key and iterates over all attending queries.

\subsection{Softmax Stability}

We compute attention with online softmax:
\begin{equation}
\text{Attention}(q_i, K, V) = \frac{\sum_{j \in \mathcal{A}(i)} \exp(q_i \cdot k_j / \sqrt{d}) \cdot v_j}{\sum_{j \in \mathcal{A}(i)} \exp(q_i \cdot k_j / \sqrt{d})}
\end{equation}

Using the max-subtraction trick for numerical stability:
\begin{equation}
m_i = \max_{j \in \mathcal{A}(i)} q_i \cdot k_j, \quad s_{ij} = \exp(q_i \cdot k_j - m_i)
\end{equation}

\section{Implementation}
\label{sec:implementation}

\subsection{Triton Kernel Design}

Our V14 kernel uses block-query tiling with block sizes $B_M = 64$ (queries per block):

\begin{algorithm}
\caption{Cron Root Attention Forward Kernel (3-Phase)}
\begin{algorithmic}
\STATE \textbf{Input:} $Q, K, V \in \mathbb{R}^{N \times d}$, block size $B_M$
\STATE \textbf{Output:} $O \in \mathbb{R}^{N \times d}$
\STATE Compute $W = \lceil\sqrt{N}\rceil$
\STATE \textbf{Pre-compute:} $\text{RK}[r] = \text{mean}(K[rW:(r{+}1)W])$, $\text{RV}[r] = \text{mean}(V[rW:(r{+}1)W])$
\FOR{each query block $i = 0, B_M, 2B_M, \ldots$}
    \STATE Load $Q[i:i+B_M]$ to SRAM
    \STATE Initialize $m = -\infty$, $\ell = 0$, $o = 0$
    \STATE // Phase 1: Local window
    \FOR{$j$ in local range $[\max(0, i-W+1), i]$}
        \STATE $s = QK^T / \sqrt{d}$, update $m, \ell, o$ with online softmax
    \ENDFOR
    \STATE // Phase 2: Strided window (before local)
    \FOR{$j \in \{0, W, 2W, \ldots\}$ where $j < \max(0, i-W+1)$}
        \STATE $s = QK^T / \sqrt{d}$, update $m, \ell, o$ with online softmax
    \ENDFOR
    \STATE // Phase 3: Relay (compressed 2-hop blocks)
    \FOR{relay block $r$ where $(r{+}1)W{-}1 < \max(0, i{-}W{+}1)$}
        \STATE $s = Q \cdot \text{RK}[r]^T / \sqrt{d}$, update $m, \ell, o$ with online softmax using $\text{RV}[r]$
    \ENDFOR
    \STATE Store $O[i:i+B_M] = o / \ell$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Optimized Backward Pass}

The backward pass computes $dQ$, $dK$, and $dV$. We provide two backward strategies selected by sequence length:

\textbf{Fully-fused single kernel (S $\leq$ 8192):} A single Triton kernel computes all gradients --- dQ via dot products with local and strided K, and dK/dV via \texttt{atomic\_add} from all contributing queries. The relay mechanism is skipped entirely (\texttt{skip\_relay} optimization), eliminating pad/reshape/mean precomputation. This reduces backward from 4 kernel launches to \textbf{1 kernel launch}, lowering the training crossover from $\sim$12K to $\sim$2K tokens.

\textbf{Key-centric multi-kernel (S $>$ 8192):} Four specialized kernels with exclusive ownership and zero atomic contention:
\begin{itemize}
    \item \textbf{dQ kernel}: Query-parallel, includes all three phases (local + strided + relay contributions).
    \item \textbf{Local dK/dV}: Each key block processes only its $O(\sqrt{N})$ local queries.
    \item \textbf{Strided dK/dV}: Each block \textit{owns} one strided key and iterates over all $O(N)$ queries. Register accumulation, single write.
    \item \textbf{Relay dK/dV}: Each block \textit{owns} one relay key/value pair. Gradient scatter via the mean-pooling Jacobian: $dK[rW{+}i] \mathrel{+}= d\text{RK}[r] / W$.
\end{itemize}

\subsection{Hardware Optimization}

Key optimizations for broad GPU support:
\begin{itemize}
    \item Dynamic SM detection via \texttt{torch.cuda.get\_device\_properties()}
    \item GPU compatibility table covering 40+ models (RTX 20/30/40/50 series, H100, H800, A100, V100)
    \item Block-query tiling with BLOCK\_M=64 queries per thread block
    \item Coalesced memory access with vectorized loads
    \item FP16 compute with FP32 accumulation for numerical stability
\end{itemize}

\section{Experiments}
\label{sec:experiments}

\subsection{Forward Pass Kernel Benchmarks}

\begin{table}[h]
\centering
\caption{Forward pass kernel latency on RTX 5070 Ti (B=1, H=8, D=64, FP16, PyTorch 2.9.1 + CUDA 12.8)}
\begin{tabular}{lrrr}
\toprule
Seq Length & SDPA/Flash (ms) & Cron Root (ms) & Speedup \\
\midrule
1,024 & 0.031 & 0.030 & 1.02x \\
2,048 & 0.109 & 0.032 & 3.44x \\
4,096 & 0.295 & 0.032 & 9.24x \\
8,192 & 0.966 & 0.072 & 13.3x \\
16,384 & 3.32 & 0.335 & 9.91x \\
32,768 & 12.8 & 1.19 & 10.8x \\
65,536 & 49.7 & 2.68 & 18.6x \\
131,072 & 197 & 7.28 & 27.0x \\
262,144 & 772 & 17.0 & 45.3x \\
524,288 & 3089 & 54.4 & \textbf{56.8x} \\
\bottomrule
\end{tabular}
\label{tab:kernel}
\end{table}

The subquadratic scaling is evident: at 524K tokens, Cron Root Attention is 57x faster than SDPA (which uses FlashAttention-2 as its backend), demonstrating the $O(N\sqrt{N})$ vs $O(N^2)$ complexity difference. The forward crossover is approximately 1K tokens. For training (forward + backward), the crossover is approximately 2K tokens thanks to the fully-fused single-kernel backward optimization.

\subsection{End-to-End Training Performance}

\begin{table}[h]
\centering
\caption{Complete forward + backward pass timing (B=1, H=8, D=64, PyTorch 2.9.1 + CUDA 12.8)}
\begin{tabular}{lrrr}
\toprule
Seq Length & Cron Root Fwd+Bwd (ms) & SDPA Fwd+Bwd (ms) & Speedup \\
\midrule
2K & 0.343 & 0.332 & 0.97x \\
4K & 0.658 & 0.954 & 1.45x \\
8K & 1.70 & 2.93 & 1.73x \\
16K & 8.66 & 12.3 & 1.42x \\
32K & 27.4 & 46.3 & 1.69x \\
64K & 65.2 & 173 & 2.65x \\
128K & 182 & 683 & \textbf{3.76x} \\
\bottomrule
\end{tabular}
\label{tab:training}
\end{table}

Training speedup increases with sequence length due to the quadratic vs subquadratic scaling gap. At 131K tokens, we achieve 3.76x end-to-end speedup. For sequences $\leq$ 8K tokens, we use a \textbf{fully-fused single-kernel backward} that computes dQ, local dK/dV, and strided dK/dV in one kernel launch (with atomic\_add for K/V gradient accumulation), reducing the training crossover from $\sim$12K to $\sim$2K tokens. For longer sequences, the relay mechanism activates and we use four specialized zero-atomic kernels. A hybrid mode auto-selects SDPA for S $<$ 1536 and Cron Root for S $\geq$ 1536, guaranteeing $\geq$1.0x speedup at all sequence lengths.

\subsection{Correctness Validation}

Comparing V14 output to sparse reference implementation:
\begin{itemize}
    \item Maximum absolute difference: 0.000977 (FP16)
    \item Determinism: Bit-exact across runs
    \item Gradient flow: Verified for dQ, dK, dV
    \item NaN/Inf check: All outputs validated
\end{itemize}

\section{Code Release}
\label{sec:code}

Cron Root Attention is available as a pip package:

\begin{verbatim}
pip install cron-root-attention
\end{verbatim}

Or from source:
\begin{verbatim}
git clone https://github.com/zitacron/cron-root-attention.git
cd cron-root-attention && pip install -e .
\end{verbatim}

Drop-in replacement:
\begin{verbatim}
from cron_root_attention import cron_root_attention, CronRootMultiheadAttention

# Functional API
output = cron_root_attention(q, k, v)  # (B, H, S, D) -> (B, H, S, D)

# Module API
attn = CronRootMultiheadAttention(embed_dim=1024, num_heads=16)
output, _ = attn(x, x, x)

# Check GPU compatibility
from cron_root_attention import get_gpu_info
print(get_gpu_info())
# {'gpu_name': 'NVIDIA GeForce RTX 5070 Ti', 'sm_count': 70, ...}
\end{verbatim}

\subsection{Supported Hardware}

The package automatically detects GPU SM count for optimal kernel grid sizing:

\begin{table}[h]
\centering
\caption{Supported GPU families}
\begin{tabular}{ll}
\toprule
Category & GPUs \\
\midrule
Blackwell (50 series) & RTX 5090, 5080, 5070 Ti, 5070 \\
Ada (40 series) & RTX 4090, 4080, 4070 Ti, 4070, 4060 Ti \\
Ampere (30 series) & RTX 3090, 3080, 3070, 3060 \\
Turing (20 series) & RTX 2080 Ti, 2080, 2070, 2060, TITAN RTX \\
Datacenter & H100, H200, H800, A100, L40S, L4, V100, B100, B200 \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}

Cron Root Attention achieves true subquadratic scaling ($O(N\sqrt{N})$) with practical speedups up to 57x for forward passes and 3.76x for complete training at long sequence lengths. Our 3-phase architecture (local + strided + relay) provides 100\% sequence coverage through a single softmax pass, eliminating the gradient dilution problem that plagues multi-hop sparse attention. The fully-fused single-kernel backward (for S $\leq$ 8K) and key-centric multi-kernel backward (for S $>$ 8K) minimize launch overhead while eliminating atomic contention at scale. A hybrid mode auto-selects SDPA below the crossover point, guaranteeing $\geq$1.0x speedup at all sequence lengths. The pip-installable package provides drop-in replacement modules for immediate adoption in long-context LLM training and inference.

\section*{Acknowledgments}

Built on Triton by OpenAI. Inspired by FlashAttention by Tri Dao.

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{flashattn}
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
\newblock FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.
\newblock {\em NeurIPS}, 2022.

\bibitem{flashattn2}
Tri Dao.
\newblock FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.
\newblock {\em arXiv:2307.08691}, 2023.

\bibitem{flashdecoding}
Tri Dao, Daniel Haziza, Francisco Massa, and Gintare Karolina Dziugaite.
\newblock Flash-Decoding for Long-Context Inference.
\newblock {\em Blog post}, 2023.

\bibitem{longformer}
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
\newblock Longformer: The Long-Document Transformer.
\newblock {\em arXiv:2004.05150}, 2020.

\bibitem{bigbird}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, et al.
\newblock Big Bird: Transformers for Longer Sequences.
\newblock {\em NeurIPS}, 2020.

\bibitem{sparse_transformers}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating Long Sequences with Sparse Transformers.
\newblock {\em arXiv:1904.10509}, 2019.

\bibitem{linear_attention}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
\newblock Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention.
\newblock {\em ICML}, 2020.

\bibitem{rwkv}
Bo Peng, Eric Alcaide, Quentin Anthony, et al.
\newblock RWKV: Reinventing RNNs for the Transformer Era.
\newblock {\em arXiv:2305.13048}, 2023.

\bibitem{mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-Time Sequence Modeling with Selective State Spaces.
\newblock {\em arXiv:2312.00752}, 2023.

\end{thebibliography}

\end{document}
