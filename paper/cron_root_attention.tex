\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}

% ArXiv preprint style
\usepackage[margin=1in]{geometry}

\title{Cron Root Attention: Subquadratic Self-Attention via Structured Sparsity}

\author{
  Zitacron\\
  \texttt{https://github.com/zitacron}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We introduce Cron Root Attention, a sparse attention mechanism that reduces the computational complexity of self-attention from $O(N^2)$ to $O(N\sqrt{N})$ while preserving causal language modeling semantics. Our method combines three phases: (1) a local sliding window of size $\sqrt{N}$, (2) a strided global window of size $\sqrt{N}$, and (3) a \textbf{relay mechanism} that carries block-mean compressed key/value summaries from $\sqrt{N}$ blocks, enabling full-sequence coverage through a single softmax pass. Each query attends to $O(3\sqrt{N})$ positions, achieving 100\% token coverage without gradient dilution. We implement optimized Triton kernels with a fully-fused single-kernel backward pass and relay-skip optimization that achieve up to \textbf{79x forward kernel speedup} at sequence length 524K and \textbf{3.6x end-to-end training speedup} at 131K tokens, with forward crossover at $\sim$2K tokens and training crossover at $\sim$2K tokens. All benchmarks were conducted on a single NVIDIA RTX 5070 Ti (Blackwell GB203, SM 12.0) using PyTorch 2.9.1, CUDA 12.8, Triton 3.5.1, and FP16 precision. Our implementation supports 40+ GPU models from consumer (RTX 20/30/40/50 series) to datacenter (H100, H800, A100, V100) hardware with automatic SM detection. We release Cron Root Attention as an open-source pip package under Apache 2.0 license.
\end{abstract}

\section{Introduction}

Transformer attention scales quadratically with sequence length, creating a fundamental bottleneck for long-context language models. While Flash Attention \cite{flashattn} and FlashAttention-2 \cite{flashattn2} optimize memory access patterns, they maintain $O(N^2)$ complexity. Sparse attention variants like Longformer \cite{longformer} and BigBird \cite{bigbird} achieve linear complexity but require fixed-size windows that don't scale with context.

We propose Cron Root Attention (named for Zitacron + Root $\sqrt{}$), which achieves true subquadratic scaling by using a window size proportional to $\sqrt{N}$. Our key insight is that combining:
\begin{enumerate}
    \item A \textbf{local window} of size $\sqrt{N}$ for recent context
    \item A \textbf{strided window} sampling every $\sqrt{N}$-th token
    \item A \textbf{relay mechanism} carrying block-mean K/V summaries from $\sqrt{N}$ blocks
\end{enumerate}
yields $O(N\sqrt{N})$ total complexity while providing \textbf{100\% sequence coverage} through a single softmax pass, eliminating the gradient dilution problem inherent in multi-hop sparse attention.

\subsection{Contributions}

\begin{itemize}
    \item We formalize the $\sqrt{N}$ attention pattern and prove its $O(N\sqrt{N})$ complexity with 2-hop universal reachability (Section \ref{sec:method}).
    \item We develop block-query tiled Triton kernels with a fully-fused single-kernel backward achieving 79x forward speedup at 524K tokens (Section \ref{sec:implementation}).
    \item We demonstrate \textbf{3.6x} end-to-end training speedup at 131K sequence length with training crossover at $\sim$2K tokens (Section \ref{sec:experiments}).
    \item We release a pip-installable package with automatic GPU detection supporting 40+ GPU models (Section \ref{sec:code}).
\end{itemize}

\section{Related Work}

\textbf{Efficient Attention.} Flash Attention \cite{flashattn} uses tiling and recomputation to reduce memory from $O(N^2)$ to $O(N)$ while maintaining exact computation. FlashAttention-2 \cite{flashattn2} improves parallelism. Flash-Decoding \cite{flashdecoding} optimizes inference. These maintain quadratic compute.

\textbf{Sparse Attention.} Longformer \cite{longformer} uses local + global tokens. BigBird \cite{bigbird} adds random attention. Sparse Transformers \cite{sparse_transformers} use strided patterns. These typically use fixed window sizes, limiting scalability.

\textbf{Linear Attention.} Linear attention \cite{linear_attention} and variants like RWKV \cite{rwkv} and Mamba \cite{mamba} achieve $O(N)$ but sacrifice expressivity.

\textbf{Our Approach.} Cron Root Attention combines the hardware optimization of Flash Attention with the scalability of sparse attention, using adaptive $\sqrt{N}$-sized windows that scale with sequence length.

\section{Method}
\label{sec:method}

\subsection{Attention Pattern}

For a sequence of length $N$, we define the window size $W = \lceil\sqrt{N}\rceil$. Each query $q_i$ attends to:

\begin{equation}
\mathcal{A}(i) = \underbrace{\{j : \max(0, i-W+1) \leq j \leq i\}}_{\text{Local window}} \cup \underbrace{\{j : j \equiv 0 \pmod{W}, j < \max(0, i-W+1)\}}_{\text{Strided window}}
\end{equation}

\subsection{Complexity Analysis}

\begin{theorem}
The total attention cost is $O(N\sqrt{N})$.
\end{theorem}

\begin{proof}
Each query attends to at most $W$ local positions and at most $\lceil N/W \rceil$ strided positions:
\begin{equation}
|\mathcal{A}(i)| \leq W + \frac{N}{W} = \sqrt{N} + \sqrt{N} = 2\sqrt{N}
\end{equation}
Summing over $N$ queries: $N \cdot 2\sqrt{N} = O(N\sqrt{N})$.
\end{proof}

\subsection{2-Hop Reachability}

\begin{theorem}
Any token can reach any other token in at most 2 attention hops.
\end{theorem}

\begin{proof}
For tokens $A$ at position $i$ and $B$ at position $j$ where $j < i - W$:
\begin{enumerate}
    \item Let $C$ be at the strided position $\lfloor j/W \rfloor \cdot W$
    \item $A$ attends to $C$ via strided window (since $C < i - W + 1$)
    \item $C$ attends to $B$ via local window (since $|C - j| < W$)
\end{enumerate}
Thus $A \rightarrow C \rightarrow B$ forms a 2-hop path.
\end{proof}

\subsection{Relay Mechanism (Phase 3)}

While the 2-hop reachability property guarantees theoretical coverage, in practice information traversing two separate softmax normalizations across layers suffers from \textbf{gradient dilution}: the gradient signal decays exponentially through each softmax bottleneck.

We introduce a relay mechanism that carries compressed 2-hop information through a \textbf{single} softmax pass. For each block $r \in \{0, 1, \ldots, \lceil N/W \rceil - 1\}$, we pre-compute:
\begin{equation}
\text{relay\_k}[r] = \frac{1}{W} \sum_{j=rW}^{(r+1)W-1} k_j, \quad \text{relay\_v}[r] = \frac{1}{W} \sum_{j=rW}^{(r+1)W-1} v_j
\end{equation}

These relay keys and values participate in the \textbf{same} softmax as the local and strided scores:
\begin{equation}
\mathcal{A}(i) = \mathcal{A}_{\text{local}}(i) \cup \mathcal{A}_{\text{strided}}(i) \cup \mathcal{A}_{\text{relay}}(i)
\end{equation}
where $\mathcal{A}_{\text{relay}}(i) = \{r : (r+1)W - 1 < \max(0, i - W + 1)\}$, i.e., relay blocks whose entire span precedes the local window.

The total attention set size remains $O(3\sqrt{N})$ per query:
\begin{equation}
|\mathcal{A}(i)| \leq W + \frac{N}{W} + \frac{N}{W} = 3\sqrt{N}
\end{equation}

The relay backward pass uses the mean-pooling Jacobian for gradient scatter:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial k_j} \mathrel{+}= \frac{1}{W} \cdot \frac{\partial \mathcal{L}}{\partial \text{relay\_k}[\lfloor j/W \rfloor]}
\end{equation}

This is implemented as a zero-atomic, exclusive-ownership relay $dK/dV$ kernel where each block owns one relay key and iterates over all attending queries.

\subsection{Softmax Stability}

We compute attention with online softmax:
\begin{equation}
\text{Attention}(q_i, K, V) = \frac{\sum_{j \in \mathcal{A}(i)} \exp(q_i \cdot k_j / \sqrt{d}) \cdot v_j}{\sum_{j \in \mathcal{A}(i)} \exp(q_i \cdot k_j / \sqrt{d})}
\end{equation}

Using the max-subtraction trick for numerical stability:
\begin{equation}
m_i = \max_{j \in \mathcal{A}(i)} q_i \cdot k_j, \quad s_{ij} = \exp(q_i \cdot k_j - m_i)
\end{equation}

\section{Implementation}
\label{sec:implementation}

\subsection{Triton Kernel Design}

Our V14 kernel uses block-query tiling with block sizes $B_M = 64$ (queries per block):

\begin{algorithm}
\caption{Cron Root Attention Forward Kernel (3-Phase)}
\begin{algorithmic}
\STATE \textbf{Input:} $Q, K, V \in \mathbb{R}^{N \times d}$, block size $B_M$
\STATE \textbf{Output:} $O \in \mathbb{R}^{N \times d}$
\STATE Compute $W = \lceil\sqrt{N}\rceil$
\STATE \textbf{Pre-compute:} $\text{RK}[r] = \text{mean}(K[rW:(r{+}1)W])$, $\text{RV}[r] = \text{mean}(V[rW:(r{+}1)W])$
\FOR{each query block $i = 0, B_M, 2B_M, \ldots$}
    \STATE Load $Q[i:i+B_M]$ to SRAM
    \STATE Initialize $m = -\infty$, $\ell = 0$, $o = 0$
    \STATE // Phase 1: Local window
    \FOR{$j$ in local range $[\max(0, i-W+1), i]$}
        \STATE $s = QK^T / \sqrt{d}$, update $m, \ell, o$ with online softmax
    \ENDFOR
    \STATE // Phase 2: Strided window (before local)
    \FOR{$j \in \{0, W, 2W, \ldots\}$ where $j < \max(0, i-W+1)$}
        \STATE $s = QK^T / \sqrt{d}$, update $m, \ell, o$ with online softmax
    \ENDFOR
    \STATE // Phase 3: Relay (compressed 2-hop blocks)
    \FOR{relay block $r$ where $(r{+}1)W{-}1 < \max(0, i{-}W{+}1)$}
        \STATE $s = Q \cdot \text{RK}[r]^T / \sqrt{d}$, update $m, \ell, o$ with online softmax using $\text{RV}[r]$
    \ENDFOR
    \STATE Store $O[i:i+B_M] = o / \ell$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Optimized Backward Pass}

The backward pass computes $dQ$, $dK$, and $dV$. We provide two backward strategies selected by sequence length:

\textbf{Fully-fused single kernel (S $\leq$ 8192):} A single Triton kernel computes all gradients --- dQ via dot products with local and strided K, and dK/dV via \texttt{atomic\_add} from all contributing queries. The relay mechanism is skipped entirely (\texttt{skip\_relay} optimization), eliminating pad/reshape/mean precomputation. This reduces backward from 4 kernel launches to \textbf{1 kernel launch}, lowering the training crossover from $\sim$12K to $\sim$2K tokens.

\textbf{Key-centric multi-kernel (S $>$ 8192):} Four specialized kernels with exclusive ownership and zero atomic contention:
\begin{itemize}
    \item \textbf{dQ kernel}: Query-parallel, includes all three phases (local + strided + relay contributions).
    \item \textbf{Local dK/dV}: Each key block processes only its $O(\sqrt{N})$ local queries.
    \item \textbf{Strided dK/dV}: Each block \textit{owns} one strided key and iterates over all $O(N)$ queries. Register accumulation, single write.
    \item \textbf{Relay dK/dV}: Each block \textit{owns} one relay key/value pair. Gradient scatter via the mean-pooling Jacobian: $dK[rW{+}i] \mathrel{+}= d\text{RK}[r] / W$.
\end{itemize}

\subsection{Hardware Optimization}

Key optimizations for broad GPU support:
\begin{itemize}
    \item Dynamic SM detection via \texttt{torch.cuda.get\_device\_properties()}
    \item GPU compatibility table covering 40+ models (RTX 20/30/40/50 series, H100, H800, A100, V100)
    \item Block-query tiling with BLOCK\_M=64 queries per thread block
    \item Coalesced memory access with vectorized loads
    \item FP16 compute with FP32 accumulation for numerical stability
\end{itemize}

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}
\label{sec:setup}

All benchmarks were conducted on the following hardware and software configuration:

\textbf{Hardware:}
\begin{itemize}
    \item GPU: NVIDIA GeForce RTX 5070 Ti (Blackwell GB203, compute capability SM~12.0, 70~SMs, 16{,}303~MiB GDDR7 VRAM)
    \item CPU: AMD Ryzen 9 7900X (12 cores / 24 threads, 5.7~GHz boost)
    \item System RAM: 32~GiB DDR5
    \item CUDA driver: 580.126.16
\end{itemize}

\textbf{Software:}
\begin{itemize}
    \item OS: Linux 6.17.0-14-generic (x86\_64)
    \item PyTorch: 2.9.1+cu128
    \item CUDA toolkit: 12.8
    \item cuDNN: 9.1.0.2
    \item Triton: 3.5.1
\end{itemize}

\textbf{Benchmark protocol:} All latencies are measured using CUDA events with \texttt{torch.cuda.synchronize()} barriers. Each configuration is run with dedicated warm-up iterations (so Triton JIT compilation is excluded from all reported times) followed by timed repetitions; the trimmed mean (10\% trim on each tail) is reported to reduce outlier sensitivity. Forward-only benchmarks record a single \texttt{forward()} call under \texttt{torch.no\_grad()}; training benchmarks additionally call \texttt{loss.backward()}. Randomized tensors (\texttt{torch.randn}, FP16) are used throughout. All runs are single-GPU, single-process, batch size $B=1$.

\textbf{Scope and limitations:} These benchmarks measure isolated attention kernel latency and do not include I/O, optimizer steps, or inter-GPU communication. They represent the best-case scenario for subquadratic attention in terms of relative advantage; wall-clock speedup in a full training loop will vary due to other bottlenecks. Results are specific to SM~12.0 (Blackwell) and will differ on other architectures (e.g., A100/H100 use SM~8.0/9.0 with different GEMM paths and different Triton tile mappings). The XL model (H=32, D=128) could not be benchmarked at $S=524{,}288$ due to the 16~GiB VRAM constraint.

\subsection{Forward Pass Kernel Benchmarks}

\begin{table}[h]
\centering
\caption{Forward pass kernel latency — RTX 5070 Ti, SM~12.0, 70~SMs (B=1, H=8, D=64, FP16, PyTorch 2.9.1, CUDA 12.8, Triton 3.5.1). See Section~\ref{sec:setup} for full setup.}
\begin{tabular}{lrrr}
\toprule
Seq Length & SDPA/Flash (ms) & Cron Root (ms) & Speedup \\
\midrule
512 & 0.024 & 0.045 & 0.53x \\
1,024 & 0.040 & 0.046 & 0.86x \\
2,048 & 0.112 & 0.058 & 1.95x \\
4,096 & 0.290 & 0.068 & 4.27x \\
8,192 & 0.916 & 0.103 & 8.90x \\
16,384 & 3.27 & 0.288 & 11.34x \\
32,768 & 12.1 & 0.953 & 12.67x \\
65,536 & 46.5 & 1.68 & 27.65x \\
131,072 & 185 & 5.69 & 32.40x \\
262,144 & 736 & 11.8 & 62.31x \\
524,288 & 2985 & 38.0 & \textbf{78.63x} \\
\bottomrule
\end{tabular}
\label{tab:kernel}
\end{table}

The subquadratic scaling is evident: at 524K tokens, Cron Root Attention is 79x faster than SDPA (which uses FlashAttention-2 as its backend), demonstrating the $O(N\sqrt{N})$ vs $O(N^2)$ complexity difference. The forward crossover is approximately 2K tokens. For training (forward + backward), the crossover is also approximately 2K tokens thanks to the fully-fused single-kernel backward optimization.

\textbf{Launch-overhead floor at small S.} Cron Root Attention reports a near-constant $\sim$0.046\,ms for both $S{=}512$ and $S{=}1{,}024$, and 0.058--0.068\,ms for $S{=}2{,}048$ and $S{=}4{,}096$. This is not an error: at these short sequence lengths the actual attention compute is negligible ($\sqrt{512} = 23$ attend positions per query), and the measured time is dominated by the \emph{fixed-cost floor} of the Triton persistent kernel launch --- grid setup, work-queue initialization, SRAM allocation, and the GPU kernel dispatch overhead itself. The kernel does no more arithmetic at $S{=}512$ than at $S{=}1{,}024$ (both fit within one work unit of the persistent grid with $B_M{=}64$), so the two measure nearly identically. SDPA/FlashAttention-2 has a lower launch floor ($\sim$0.024\,ms) because its CUDA-native implementation avoids the Triton JIT dispatcher overhead. The result is that Cron Root is slower below $\sim$2K tokens, after which the $O(N\sqrt{N})$ vs $O(N^2)$ compute gap overtakes the fixed launch differential.

\textbf{Sub-linear speedup growth at S=131K.} The forward speedup at S=131,072 (32.40x) is only marginally above S=65,536 (27.65x), despite a $2\times$ increase in sequence length --- well below the 62.31x at S=262,144. The speedup is still \emph{monotonically increasing} throughout; it is the \emph{rate of growth} that stalls at 131K. This slowdown is a consequence of $\sqrt{N}$ alignment. The window size $W = \lceil\sqrt{N}\rceil$ yields $W=363$ for $N=131{,}072$, a non-power-of-2 value, while adjacent benchmarks yield perfect powers of two ($W=256$ at $N=65{,}536$ and $W=512$ at $N=262{,}144$). This causes three compounding penalties:

\begin{enumerate}
    \item \textbf{Loop tiling waste}: The kernel iterates in blocks of 64 (BLOCK\_M). With $W=363$, the inner loop requires $\lceil(363{+}64)/64\rceil = 7$ iterations per tile vs 5 for $W=256$, and $N=131{,}072$ requires $2\times$ more total tiles than $N=65{,}536$ --- yielding $2 \times 7/5 = 2.8\times$ the theoretical tile work (vs a $2\times$ increase for the $4\times$ larger $N=262{,}144$ which has $W=512$, i.e., 9 iterations per tile but $2\times$ tiles).
    \item \textbf{Relay precompute penalty}: For $S > 8{,}192$, relay keys/values require a reshape to $[B, H, \lfloor N/W \rfloor, W, D]$. At $N=131{,}072$, this is $[1, H, 362, 363, D]$---a mean over 363 non-power-of-2 elements, requiring 334 tokens of zero-padding before reshape (vs zero padding at $N=65{,}536$ and $N=262{,}144$ where $W$ evenly divides $N$).
    \item \textbf{Relay tensor misalignment}: The relay dimensions $362 \times 363$ are both non-aligned, causing suboptimal memory access patterns compared to the clean $256 \times 256$ and $512 \times 512$ layouts at adjacent sequence lengths.
\end{enumerate}

All three penalties compound simultaneously at $S=131{,}072$ specifically. At $S=262{,}144$, $W=512$ is a perfect power of two and the speedup recovers to 62.31x. A potential optimization is rounding $W$ to the next power of two, trading $O(\sqrt{N})$ extra attend positions ($<$41\% overhead at 131K) for aligned tiling, though this would change the attention pattern semantics.

\subsection{End-to-End Training Performance}

\begin{table}[h]
\centering
\caption{Complete forward + backward pass timing — RTX 5070 Ti, SM~12.0 (B=1, H=8, D=64, FP16, PyTorch 2.9.1, CUDA 12.8, Triton 3.5.1). Rows $\leq$8K use the fully-fused single-kernel backward; rows $>$8K use the atomic-free 3-kernel split backward. See Section~\ref{sec:setup}.}
\begin{tabular}{lrrr}
\toprule
Seq Length & Cron Root Fwd+Bwd (ms) & SDPA Fwd+Bwd (ms) & Speedup \\
\midrule
512 & 0.224 & 0.134 & 0.60x \\
1K & 0.228 & 0.164 & 0.72x \\
2K & 0.299 & 0.344 & 1.15x \\
4K & 0.503 & 0.920 & 1.83x \\
8K & 0.946 & 2.91 & 3.07x \\
16K & 8.47 & 10.82 & 1.28x \\
32K & 23.66 & 41.67 & 1.76x \\
64K & 63.51 & 163.18 & 2.57x \\
128K & 178.0 & 648.6 & \textbf{3.64x} \\
\bottomrule
\end{tabular}
\label{tab:training}
\end{table}

Training speedup increases with sequence length due to the quadratic vs subquadratic scaling gap. At 131K tokens, we achieve \textbf{3.64x} end-to-end speedup. For sequences $\leq$ 8K tokens, we use a \textbf{fully-fused single-kernel backward} that computes dQ, local dK/dV, and strided dK/dV in one kernel launch (with atomic\_add for K/V gradient accumulation), achieving up to 3.07x speedup at 8K. For sequences $>$ 8K, we use the \textbf{atomic-free 3-kernel split backward}: separate zero-atomic kernels for dQ, dK/dV, and relay dK/dV, eliminating inter-warp atomic contention and delivering approximately $2\times$ backward throughput improvement over the previous implementation at these sequence lengths.

\textbf{Training dip at S=16K.} At the 8K$\to$16K transition, the backward switches from the fully-fused single-kernel to the atomic-free 3-kernel split strategy. This causes a visible throughput dip from 3.07x at 8K to 1.28x at 16K. Two factors contribute: (1) the multi-kernel launch overhead is amortized poorly at 16K because the per-kernel work is still relatively small, and (2) the relay mechanism activates for $S > 8{,}192$, adding the relay precompute (pad, reshape, mean) and a separate backward kernel for relay dK/dV gradients. As sequence length increases beyond 16K, the subquadratic compute advantage dominates these fixed costs and the speedup resumes its upward trajectory (1.76x at 32K, 2.57x at 64K, 3.64x at 128K).

For longer sequences, we use four specialized zero-atomic kernels. A hybrid mode auto-selects SDPA for S $<$ 1536 and Cron Root for S $\geq$ 1536, guaranteeing $\geq$1.0x speedup at all sequence lengths.

\subsection{Inference Performance}

\begin{table}[h]
\centering
\caption{Inference (\texttt{no\_grad} prefill) latency — RTX 5070 Ti, SM~12.0 (B=1, H=8, D=64, FP16, PyTorch 2.9.1, CUDA 12.8, Triton 3.5.1). See Section~\ref{sec:setup}.}
\begin{tabular}{lrrr}
\toprule
Seq Length & SDPA (ms) & Cron Root (ms) & Speedup \\
\midrule
512 & 0.025 & 0.046 & 0.54x \\
1,024 & 0.037 & 0.050 & 0.75x \\
2,048 & 0.110 & 0.050 & 2.21x \\
4,096 & 0.292 & 0.068 & 4.27x \\
8,192 & 0.917 & 0.108 & 8.52x \\
16,384 & 3.29 & 0.279 & 11.82x \\
32,768 & 12.1 & 0.954 & 12.70x \\
65,536 & 46.9 & 1.70 & 27.60x \\
131,072 & 185 & 5.71 & 32.45x \\
262,144 & 737 & 11.8 & 62.35x \\
524,288 & 2986 & 38.1 & \textbf{78.41x} \\
\bottomrule
\end{tabular}
\label{tab:inference}
\end{table}

Under \texttt{torch.no\_grad()} inference, Cron Root Attention exhibits performance characteristics nearly identical to the forward-pass kernel benchmarks (Table~\ref{tab:kernel}), with a crossover at $\sim$2K tokens and a peak speedup of \textbf{78.41x} at 524K. This is expected: the Triton kernels execute identically regardless of PyTorch's autograd context, so \texttt{no\_grad()} only eliminates the negligible overhead of building the computation graph. The same $\sqrt{N}$-alignment slowdown at S=131,072 (32.45x, up from 27.60x at 65K) is present, recovering to 62.35x at S=262,144 ($W=512$, a perfect power of two).
\label{sec:inference}

\subsection{Correctness Validation}

Comparing V14 output to sparse reference implementation:
\begin{itemize}
    \item Maximum absolute difference: 0.000977 (FP16)
    \item Determinism: Bit-exact across runs
    \item Gradient flow: Verified for dQ, dK, dV
    \item NaN/Inf check: All outputs validated
\end{itemize}

\section{Code Release}
\label{sec:code}

Cron Root Attention is available as a pip package:

\begin{verbatim}
pip install cron-root-attention
\end{verbatim}

Or from source:
\begin{verbatim}
git clone https://github.com/zitacron/cron-root-attention.git
cd cron-root-attention && pip install -e .
\end{verbatim}

Drop-in replacement:
\begin{verbatim}
from cron_root_attention import cron_root_attention, CronRootMultiheadAttention

# Functional API
output = cron_root_attention(q, k, v)  # (B, H, S, D) -> (B, H, S, D)

# Module API
attn = CronRootMultiheadAttention(embed_dim=1024, num_heads=16)
output, _ = attn(x, x, x)

# Check GPU compatibility
from cron_root_attention import get_gpu_info
print(get_gpu_info())
# {'gpu_name': 'NVIDIA GeForce RTX 5070 Ti', 'sm_count': 70, ...}
\end{verbatim}

\subsection{Supported Hardware}

The package automatically detects GPU SM count for optimal kernel grid sizing:

\begin{table}[h]
\centering
\caption{Supported GPU families}
\begin{tabular}{ll}
\toprule
Category & GPUs \\
\midrule
Blackwell (50 series) & RTX 5090, 5080, 5070 Ti, 5070 \\
Ada (40 series) & RTX 4090, 4080, 4070 Ti, 4070, 4060 Ti \\
Ampere (30 series) & RTX 3090, 3080, 3070, 3060 \\
Turing (20 series) & RTX 2080 Ti, 2080, 2070, 2060, TITAN RTX \\
Datacenter & H100, H200, H800, A100, L40S, L4, V100, B100, B200 \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}

Cron Root Attention achieves true subquadratic scaling ($O(N\sqrt{N})$) with practical speedups up to 79x for forward passes and \textbf{3.6x} for complete training at long sequence lengths. Our 3-phase architecture (local + strided + relay) provides 100\% sequence coverage through a single softmax pass, eliminating the gradient dilution problem that plagues multi-hop sparse attention. The fully-fused single-kernel backward (for $S \leq 8$K) achieves up to 3.07x training speedup, while the atomic-free 3-kernel split backward (for $S > 8$K) scales to 3.64x at 131K tokens by eliminating inter-warp atomic contention. A hybrid mode auto-selects SDPA below the crossover point, guaranteeing $\geq$1.0x speedup at all sequence lengths. All benchmarks were conducted on a single RTX 5070 Ti (Blackwell, SM~12.0) running PyTorch 2.9.1 + CUDA 12.8 + Triton 3.5.1; see Section~\ref{sec:setup} for the complete experimental setup. The pip-installable package provides drop-in replacement modules for immediate adoption in long-context LLM training and inference.

\section*{Acknowledgments}

Built on Triton by OpenAI. Inspired by FlashAttention by Tri Dao.

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{flashattn}
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
\newblock FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.
\newblock {\em NeurIPS}, 2022.

\bibitem{flashattn2}
Tri Dao.
\newblock FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.
\newblock {\em arXiv:2307.08691}, 2023.

\bibitem{flashdecoding}
Tri Dao, Daniel Haziza, Francisco Massa, and Gintare Karolina Dziugaite.
\newblock Flash-Decoding for Long-Context Inference.
\newblock {\em Blog post}, 2023.

\bibitem{longformer}
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
\newblock Longformer: The Long-Document Transformer.
\newblock {\em arXiv:2004.05150}, 2020.

\bibitem{bigbird}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, et al.
\newblock Big Bird: Transformers for Longer Sequences.
\newblock {\em NeurIPS}, 2020.

\bibitem{sparse_transformers}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating Long Sequences with Sparse Transformers.
\newblock {\em arXiv:1904.10509}, 2019.

\bibitem{linear_attention}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
\newblock Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention.
\newblock {\em ICML}, 2020.

\bibitem{rwkv}
Bo Peng, Eric Alcaide, Quentin Anthony, et al.
\newblock RWKV: Reinventing RNNs for the Transformer Era.
\newblock {\em arXiv:2305.13048}, 2023.

\bibitem{mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-Time Sequence Modeling with Selective State Spaces.
\newblock {\em arXiv:2312.00752}, 2023.

\end{thebibliography}

\end{document}
